{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b1c82e",
   "metadata": {},
   "source": [
    "# Late Fusion Stream Training (Playground)\n",
    "\n",
    "Runs all four streams (J, B, JM, BM) for a given fold, saves sorted `score_eval.npz` per stream in `workdir/fold_{ID}_{STREAM}`, and fuses them at the end. Use `evaluate_only=True` to skip training and just run eval on existing checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "id": "7f44f0a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:35:14.831365Z",
     "start_time": "2025-11-29T05:35:14.199865Z"
    }
   },
   "source": [
    "import os, yaml, numpy as np, torch, subprocess\n",
    "from types import SimpleNamespace\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from src.dataset import create as create_dataset\n",
    "from src.model.MPGCN import MPGCN\n",
    "from src.scheduler import create as create_scheduler\n",
    "\n",
    "fold_id = \"06\"  # e.g., \"06\"\n",
    "streams = [\"J\", \"B\", \"JM\", \"BM\"]\n",
    "evaluate_only = False  # set True to skip training\n",
    "\n",
    "print(\"Fold:\", fold_id)\n",
    "print(\"Streams:\", streams)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 06\n",
      "Streams: ['J', 'B', 'JM', 'BM']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "e202816c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:35:17.029825Z",
     "start_time": "2025-11-29T05:35:17.015876Z"
    }
   },
   "source": [
    "\n",
    "def run_stream(stream_type):\n",
    "    config_template = f\"config/playground/mpgcn_{stream_type}.yaml\"\n",
    "    with open(config_template, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    # Replace fold placeholder in paths\n",
    "    cfg[\"dataset_args\"][\"root_folder\"] = cfg[\"dataset_args\"][\"root_folder\"].replace(\"{ID}\", fold_id)\n",
    "    cfg[\"dataset_args\"][\"object_folder\"] = cfg[\"dataset_args\"][\"object_folder\"].replace(\"{ID}\", fold_id)\n",
    "    cfg[\"dataset_args\"][\"stream_type\"] = stream_type\n",
    "    cfg[\"dataset_args\"][\"fold_id\"] = fold_id\n",
    "    cfg[\"work_dir\"] = f\"./workdir/fold_{fold_id}_{stream_type}\"\n",
    "    cfg.setdefault(\"scheduler_args\", {}).setdefault(cfg[\"lr_scheduler\"], {})[\"max_epoch\"] = 100\n",
    "\n",
    "    # Convert to namespaces\n",
    "    args = SimpleNamespace(**cfg)\n",
    "    args.dataset_args = SimpleNamespace(**args.dataset_args)\n",
    "    args.model_args = SimpleNamespace(**args.model_args)\n",
    "    args.optimizer_args = SimpleNamespace(**args.optimizer_args)\n",
    "    args.scheduler_args = SimpleNamespace(**args.scheduler_args)\n",
    "\n",
    "    # Dataset\n",
    "    feeders, data_shape, num_class, A, parts = create_dataset(\n",
    "        args.dataset,\n",
    "        debug=False,\n",
    "        **vars(args.dataset_args)\n",
    "    )\n",
    "    print(f\"[{stream_type}] train samples: {len(feeders['train'])}, eval samples: {len(feeders['eval'])}\")\n",
    "\n",
    "    # Sampler (no replacement)\n",
    "    train_labels = np.array([int(y) for _, y, _, _ in feeders[\"train\"]])\n",
    "    class_counts = np.bincount(train_labels, minlength=num_class) + 1e-6\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = class_weights[train_labels]\n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=torch.tensor(sample_weights, dtype=torch.float32),\n",
    "        num_samples=len(train_labels),\n",
    "        replacement=False,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        feeders[\"train\"],\n",
    "        batch_size=args.dataset_args.train_batch_size,\n",
    "        num_workers=4 * len(args.gpus),\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    eval_loader = DataLoader(\n",
    "        feeders[\"eval\"],\n",
    "        batch_size=args.dataset_args.eval_batch_size,\n",
    "        num_workers=4 * len(args.gpus),\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # Model/optim/scheduler\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MPGCN(\n",
    "        data_shape=data_shape,\n",
    "        num_class=num_class,\n",
    "        A=torch.Tensor(A),\n",
    "        parts=parts,\n",
    "        num_areas=int(np.max(feeders[\"train\"].area_ids)) + 1,\n",
    "        **vars(args.model_args),\n",
    "    ).to(device)\n",
    "\n",
    "    def _to_dict(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "        return vars(obj)\n",
    "\n",
    "    optimizer_cfg = _to_dict(getattr(args.optimizer_args, args.optimizer))\n",
    "    optimizer_cls = getattr(torch.optim, args.optimizer)\n",
    "    optimizer = optimizer_cls(model.parameters(), **optimizer_cfg)\n",
    "\n",
    "    sched_cfg = _to_dict(getattr(args.scheduler_args, args.lr_scheduler))\n",
    "    lr_scheduler = create_scheduler(args.lr_scheduler, len(train_loader), **sched_cfg)\n",
    "    eval_interval, lr_lambda = lr_scheduler.get_lambda()\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    max_epoch = sched_cfg.get(\"max_epoch\", 100)\n",
    "    best_acc = 0\n",
    "    save_dir = args.work_dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if not evaluate_only:\n",
    "        for epoch in range(max_epoch):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, target, _, area_id in train_loader:\n",
    "                assert data.dim() == 5, f\"Expected 5D (N, C, T, V, M), got {data.shape}\"\n",
    "                assert area_id.dim() == 1, f\"Bad area_id shape {area_id.shape}\"\n",
    "                if data.dim() == 5:\n",
    "                    data = data.unsqueeze(1)\n",
    "                data = data.float().to(device)\n",
    "                target = target.long().to(device)\n",
    "                area_id = area_id.long().to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                out, _ = model(data, area_id)\n",
    "                loss = criterion(out, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                preds = out.argmax(1)\n",
    "                correct += preds.eq(target).sum().item()\n",
    "                total += target.size(0)\n",
    "\n",
    "            train_acc = correct / total\n",
    "            print(f\"[{stream_type}] Epoch {epoch+1}/{max_epoch} train_loss={running_loss/total:.4f} acc={train_acc:.4f}\")\n",
    "\n",
    "            model.eval()\n",
    "            eval_correct, eval_total = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for data, target, _, area_id in eval_loader:\n",
    "                    assert data.dim() == 5, f\"Expected 5D (N, C, T, V, M), got {data.shape}\"\n",
    "                    assert area_id.dim() == 1, f\"Bad area_id shape {area_id.shape}\"\n",
    "                    if data.dim() == 5:\n",
    "                        data = data.unsqueeze(1)\n",
    "                    data = data.float().to(device)\n",
    "                    target = target.long().to(device)\n",
    "                    area_id = area_id.long().to(device)\n",
    "                    out, _ = model(data, area_id)\n",
    "                    preds = out.argmax(1)\n",
    "                    eval_correct += preds.eq(target).sum().item()\n",
    "                    eval_total += target.size(0)\n",
    "            eval_acc = eval_correct / eval_total\n",
    "            print(f\"  Eval acc={eval_acc:.4f}\")\n",
    "\n",
    "            if eval_acc > best_acc:\n",
    "                best_acc = eval_acc\n",
    "                torch.save({\"model\": model.state_dict()}, os.path.join(save_dir, \"best.pth.tar\"))\n",
    "                print(\"  Saved best model\")\n",
    "    else:\n",
    "        print(f\"[{stream_type}] Evaluate-only mode: skipping training\")\n",
    "\n",
    "    # Deterministic eval + save score\n",
    "    best_ckpt = os.path.join(save_dir, \"best.pth.tar\")\n",
    "    if os.path.exists(best_ckpt):\n",
    "        state = torch.load(best_ckpt, map_location=\"cpu\")\n",
    "        model.load_state_dict(state[\"model\"])\n",
    "        print(f\"[{stream_type}] Loaded best checkpoint\")\n",
    "    else:\n",
    "        print(f\"[{stream_type}] Warning: best checkpoint not found, using current weights\")\n",
    "\n",
    "    model.eval()\n",
    "    all_logits, all_labels, all_names = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target, names, area_id in eval_loader:\n",
    "            assert data.dim() == 5, f\"Expected 5D (N, C, T, V, M), got {data.shape}\"\n",
    "            assert area_id.dim() == 1, f\"Bad area_id shape {area_id.shape}\"\n",
    "            if data.dim() == 5:\n",
    "                data = data.unsqueeze(1)\n",
    "            data = data.float().to(device)\n",
    "            target = target.long().to(device)\n",
    "            area_id = area_id.long().to(device)\n",
    "            out, _ = model(data, area_id)\n",
    "            all_logits.append(out.cpu().numpy())\n",
    "            all_labels.append(target.cpu().numpy())\n",
    "            all_names.extend(names)\n",
    "\n",
    "    logits_arr = np.concatenate(all_logits, axis=0)\n",
    "    labels_arr = np.concatenate(all_labels, axis=0)\n",
    "    names_arr = np.array(all_names)\n",
    "\n",
    "    np.savez(os.path.join(args.work_dir, \"score_eval.npz\"), logits=logits_arr, labels=labels_arr, names=names_arr)\n",
    "    print(f\"[{stream_type}] Saved score_eval.npz to {args.work_dir}\")\n",
    "\n",
    "    return args.work_dir\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "4baa27c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T06:43:41.068443Z",
     "start_time": "2025-11-29T05:35:21.285925Z"
    }
   },
   "source": [
    "# Run all streams\n",
    "work_dirs = []\n",
    "for st in streams:\n",
    "    wd = run_stream(st)\n",
    "    work_dirs.append(wd)\n",
    "print(\"Work dirs:\", work_dirs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[J] train samples: 802, eval samples: 284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexeidelgado/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[J] Epoch 1/100 train_loss=1.5246 acc=0.5125\n",
      "  Eval acc=0.6056\n",
      "  Saved best model\n",
      "[J] Epoch 2/100 train_loss=1.9267 acc=0.5088\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 3/100 train_loss=1.4181 acc=0.5150\n",
      "  Eval acc=0.6021\n",
      "[J] Epoch 4/100 train_loss=1.1023 acc=0.5613\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 5/100 train_loss=1.2250 acc=0.4938\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 6/100 train_loss=1.1449 acc=0.5450\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 7/100 train_loss=1.0718 acc=0.5350\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 8/100 train_loss=1.0871 acc=0.5800\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 9/100 train_loss=1.0549 acc=0.5825\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 10/100 train_loss=1.0141 acc=0.5863\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 11/100 train_loss=1.0143 acc=0.5837\n",
      "  Eval acc=0.6092\n",
      "  Saved best model\n",
      "[J] Epoch 12/100 train_loss=1.0372 acc=0.5663\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 13/100 train_loss=1.0160 acc=0.5475\n",
      "  Eval acc=0.6092\n",
      "[J] Epoch 14/100 train_loss=1.0266 acc=0.5813\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 15/100 train_loss=1.0034 acc=0.5763\n",
      "  Eval acc=0.6373\n",
      "  Saved best model\n",
      "[J] Epoch 16/100 train_loss=1.0026 acc=0.5700\n",
      "  Eval acc=0.6268\n",
      "[J] Epoch 17/100 train_loss=0.9618 acc=0.5563\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 18/100 train_loss=1.0182 acc=0.5525\n",
      "  Eval acc=0.6092\n",
      "[J] Epoch 19/100 train_loss=1.0180 acc=0.5475\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 20/100 train_loss=1.0422 acc=0.5563\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 21/100 train_loss=0.9584 acc=0.5850\n",
      "  Eval acc=0.6162\n",
      "[J] Epoch 22/100 train_loss=1.0571 acc=0.5763\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 23/100 train_loss=0.9316 acc=0.5825\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 24/100 train_loss=1.0261 acc=0.5625\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 25/100 train_loss=0.9616 acc=0.5687\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 26/100 train_loss=1.0306 acc=0.5637\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 27/100 train_loss=1.0072 acc=0.5637\n",
      "  Eval acc=0.6373\n",
      "[J] Epoch 28/100 train_loss=1.0184 acc=0.5387\n",
      "  Eval acc=0.6303\n",
      "[J] Epoch 29/100 train_loss=0.9672 acc=0.5913\n",
      "  Eval acc=0.6092\n",
      "[J] Epoch 30/100 train_loss=0.9723 acc=0.5800\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 31/100 train_loss=0.9611 acc=0.6075\n",
      "  Eval acc=0.6021\n",
      "[J] Epoch 32/100 train_loss=1.0352 acc=0.5700\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 33/100 train_loss=0.9770 acc=0.5825\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 34/100 train_loss=1.0350 acc=0.5825\n",
      "  Eval acc=0.6162\n",
      "[J] Epoch 35/100 train_loss=0.9783 acc=0.5587\n",
      "  Eval acc=0.6444\n",
      "  Saved best model\n",
      "[J] Epoch 36/100 train_loss=0.9991 acc=0.5613\n",
      "  Eval acc=0.6056\n",
      "[J] Epoch 37/100 train_loss=1.0108 acc=0.5513\n",
      "  Eval acc=0.6162\n",
      "[J] Epoch 38/100 train_loss=0.9934 acc=0.5663\n",
      "  Eval acc=0.5951\n",
      "[J] Epoch 39/100 train_loss=0.9895 acc=0.5625\n",
      "  Eval acc=0.6056\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da7fd7cb",
   "metadata": {},
   "source": [
    "## Fuse streams with ensemble script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb93207",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\n",
    "    \"python\", \"script/ensemble_playground.py\",\n",
    "    \"--fold\", fold_id,\n",
    "    \"--streams\", \"J\", \"B\", \"JM\", \"BM\",\n",
    "    \"--workdir\", \"./workdir\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7714e31",
   "metadata": {},
   "source": [
    "## Optional: in-notebook mean fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a6e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = [\"J\", \"B\", \"JM\", \"BM\"]\n",
    "scores = []\n",
    "labels_ref = None\n",
    "names_ref = None\n",
    "for s in streams:\n",
    "    path = f\"./workdir/fold_{fold_id}_{s}/score_eval.npz\"\n",
    "    d = np.load(path)\n",
    "    if labels_ref is None:\n",
    "        labels_ref = d[\"labels\"]\n",
    "        names_ref = d[\"names\"]\n",
    "    else:\n",
    "        if not np.array_equal(labels_ref, d[\"labels\"]):\n",
    "            raise ValueError(\"Label mismatch across streams\")\n",
    "        if not np.array_equal(names_ref, d[\"names\"]):\n",
    "            raise ValueError(\"Name/order mismatch across streams\")\n",
    "    scores.append(d[\"logits\"])\n",
    "\n",
    "fused = sum(scores) / len(scores)\n",
    "preds = fused.argmax(1)\n",
    "final_acc = (preds == labels_ref).mean()\n",
    "print(\"Late-fusion accuracy (mean weights):\", final_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed4904f",
   "metadata": {},
   "source": [
    "## Summary table template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff2fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "summary = [\n",
    "    {\"Stream\": \"J\", \"Acc\": None},\n",
    "    {\"Stream\": \"B\", \"Acc\": None},\n",
    "    {\"Stream\": \"JM\", \"Acc\": None},\n",
    "    {\"Stream\": \"BM\", \"Acc\": None},\n",
    "    {\"Stream\": \"Fusion\", \"Acc\": None},\n",
    "]\n",
    "pd.DataFrame(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
