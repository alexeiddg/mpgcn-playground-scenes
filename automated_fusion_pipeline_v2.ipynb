{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Automated Multi-Fold Multi-Stream Training and Fusion Pipeline\n",
    "\n",
    "This notebook automates the training and evaluation process for all 25 folds and 4 streams (J, B, JM, BM) of the MP-GCN model on the Playground dataset. It:\n",
    "\n",
    "1. Runs all 25 folds automatically (one at a time with 5-minute cooldown between folds)\n",
    "2. Trains all 4 streams per fold (J, B, JM, BM) with 1-minute cooldown between streams\n",
    "3. Performs per-fold late fusion\n",
    "4. Computes which fold yields the best fused accuracy\n",
    "5. Creates a fully automatic pipeline that runs everything in one execution\n",
    "6. Redirects all logs to files in /workdir/logs\n"
   ],
   "id": "2bde5300325646fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "from types import SimpleNamespace\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from src.dataset import create as create_dataset\n",
    "from src.model.MPGCN import MPGCN\n",
    "from src.scheduler import create as create_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from io import StringIO\n"
   ],
   "id": "69bc159a3a3b7655"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)"
   ],
   "id": "3c4410c514058300"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ],
   "id": "ab2839235d9bd536"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration and Logging Setup\n",
   "id": "d2c435f6be105a12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Global configuration\n",
    "ALL_FOLDS = [f\"{i:02d}\" for i in range(25)]  # 00-24\n",
    "ALL_STREAMS = [\"J\", \"B\", \"JM\", \"BM\"]\n",
    "EVALUATE_ONLY = False  # Set True to skip training and just run eval on existing checkpoints\n",
    "BASE_WORKDIR = \"./workdir\"\n",
    "LOGS_DIR = os.path.join(BASE_WORKDIR, \"logs\")\n",
    "\n",
    "# Create logs directory\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# Create results directory for this run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RESULTS_DIR = os.path.join(BASE_WORKDIR, f\"fusion_results_{timestamp}\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "def setup_logger(name, log_file, level=logging.INFO):\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Create logger\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Main logger\n",
    "main_log_file = os.path.join(LOGS_DIR, f\"pipeline_{timestamp}.log\")\n",
    "main_logger = setup_logger('main', main_log_file)\n",
    "\n",
    "# Only display critical messages in the notebook\n",
    "notebook_handler = logging.StreamHandler(sys.stdout)\n",
    "notebook_handler.setLevel(logging.CRITICAL)\n",
    "main_logger.addHandler(notebook_handler)\n",
    "\n",
    "# Log initial configuration\n",
    "main_logger.info(f\"Starting pipeline with timestamp: {timestamp}\")\n",
    "main_logger.info(f\"Processing {len(ALL_FOLDS)} folds: {ALL_FOLDS}\")\n",
    "main_logger.info(f\"Processing {len(ALL_STREAMS)} streams: {ALL_STREAMS}\")\n",
    "main_logger.info(f\"Evaluate only mode: {EVALUATE_ONLY}\")\n",
    "main_logger.info(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "main_logger.info(f\"Logs will be saved to: {LOGS_DIR}\")\n",
    "\n",
    "# Print minimal info to notebook\n",
    "print(f\"Pipeline started. Logs will be saved to: {LOGS_DIR}\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")"
   ],
   "id": "35cc970076caed4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stream Training Function\n",
   "id": "c1479396ce182fee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_stream(fold_id, stream_type, evaluate_only=False):\n",
    "    \"\"\"Train and evaluate a single stream for a specific fold\n",
    "    \n",
    "    Args:\n",
    "        fold_id (str): Fold ID (e.g., \"06\")\n",
    "        stream_type (str): Stream type (\"J\", \"B\", \"JM\", \"BM\")\n",
    "        evaluate_only (bool): If True, skip training and just evaluate\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (work_dir, accuracy) - Directory with results and evaluation accuracy\n",
    "    \"\"\"\n",
    "    # Setup stream-specific logger\n",
    "    log_file = os.path.join(LOGS_DIR, f\"fold_{fold_id}_{stream_type}.log\")\n",
    "    logger = setup_logger(f'fold_{fold_id}_{stream_type}', log_file)\n",
    "    \n",
    "    logger.info(f\"Starting processing for fold {fold_id}, stream {stream_type}\")\n",
    "    \n",
    "    # Redirect stdout and stderr to the log file\n",
    "    log_capture = StringIO()\n",
    "    with redirect_stdout(log_capture), redirect_stderr(log_capture):\n",
    "        try:\n",
    "            config_template = f\"config/playground/mpgcn_{stream_type}.yaml\"\n",
    "            with open(config_template, \"r\") as f:\n",
    "                cfg = yaml.safe_load(f)\n",
    "\n",
    "            # Replace fold placeholder in paths\n",
    "            cfg[\"dataset_args\"][\"root_folder\"] = cfg[\"dataset_args\"][\"root_folder\"].replace(\"{ID}\", fold_id)\n",
    "            cfg[\"dataset_args\"][\"object_folder\"] = cfg[\"dataset_args\"][\"object_folder\"].replace(\"{ID}\", fold_id)\n",
    "            cfg[\"dataset_args\"][\"stream_type\"] = stream_type\n",
    "            cfg[\"dataset_args\"][\"fold_id\"] = fold_id\n",
    "            cfg[\"work_dir\"] = f\"{BASE_WORKDIR}/fold_{fold_id}_{stream_type}\"\n",
    "            cfg.setdefault(\"scheduler_args\", {}).setdefault(cfg[\"lr_scheduler\"], {})[\"max_epoch\"] = 100\n",
    "\n",
    "            # Convert to namespaces\n",
    "            args = SimpleNamespace(**cfg)\n",
    "            args.dataset_args = SimpleNamespace(**args.dataset_args)\n",
    "            args.model_args = SimpleNamespace(**args.model_args)\n",
    "            args.optimizer_args = SimpleNamespace(**args.optimizer_args)\n",
    "            args.scheduler_args = SimpleNamespace(**args.scheduler_args)\n",
    "\n",
    "            # Dataset\n",
    "            logger.info(\"Creating dataset\")\n",
    "            feeders, data_shape, num_class, A, parts = create_dataset(\n",
    "                args.dataset,\n",
    "                debug=False,\n",
    "                **vars(args.dataset_args)\n",
    "            )\n",
    "            logger.info(f\"Train samples: {len(feeders['train'])}, eval samples: {len(feeders['eval'])}\")\n",
    "\n",
    "            # Sampler (no replacement)\n",
    "            train_labels = np.array([int(y) for _, y, _, _ in feeders[\"train\"]])\n",
    "            class_counts = np.bincount(train_labels, minlength=num_class) + 1e-6\n",
    "            class_weights = 1.0 / class_counts\n",
    "            sample_weights = class_weights[train_labels]\n",
    "            train_sampler = WeightedRandomSampler(\n",
    "                weights=torch.tensor(sample_weights, dtype=torch.float32),\n",
    "                num_samples=len(train_labels),\n",
    "                replacement=False,\n",
    "            )\n",
    "\n",
    "            train_loader = DataLoader(\n",
    "                feeders[\"train\"],\n",
    "                batch_size=args.dataset_args.train_batch_size,\n",
    "                num_workers=4 * len(args.gpus),\n",
    "                pin_memory=True,\n",
    "                sampler=train_sampler,\n",
    "                shuffle=False,\n",
    "                drop_last=True,\n",
    "            )\n",
    "\n",
    "            eval_loader = DataLoader(\n",
    "                feeders[\"eval\"],\n",
    "                batch_size=args.dataset_args.eval_batch_size,\n",
    "                num_workers=4 * len(args.gpus),\n",
    "                pin_memory=True,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "            )\n",
    "\n",
    "            # Model/optim/scheduler\n",
    "            logger.info(\"Setting up model, optimizer, and scheduler\")\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model = MPGCN(\n",
    "                data_shape=data_shape,\n",
    "                num_class=num_class,\n",
    "                A=torch.Tensor(A),\n",
    "                parts=parts,\n",
    "                num_areas=int(np.max(feeders[\"train\"].area_ids)) + 1,\n",
    "                **vars(args.model_args),\n",
    "            ).to(device)\n",
    "\n",
    "            def _to_dict(obj):\n",
    "                if isinstance(obj, dict):\n",
    "                    return obj\n",
    "                return vars(obj)\n",
    "\n",
    "            optimizer_cfg = _to_dict(getattr(args.optimizer_args, args.optimizer))\n",
    "            optimizer_cls = getattr(torch.optim, args.optimizer)\n",
    "            optimizer = optimizer_cls(model.parameters(), **optimizer_cfg)\n",
    "\n",
    "            sched_cfg = _to_dict(getattr(args.scheduler_args, args.lr_scheduler))\n",
    "            lr_scheduler = create_scheduler(args.lr_scheduler, len(train_loader), **sched_cfg)\n",
    "            eval_interval, lr_lambda = lr_scheduler.get_lambda()\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "            max_epoch = sched_cfg.get(\"max_epoch\", 100)\n",
    "            best_acc = 0\n",
    "            save_dir = args.work_dir\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            if not evaluate_only:\n",
    "                logger.info(f\"Starting training for {max_epoch} epochs\")\n",
    "                for epoch in range(max_epoch):\n",
    "                    model.train()\n",
    "                    running_loss = 0.0\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    for data, target, _, area_id in train_loader:\n",
    "                        assert data.dim() == 5, f\"Expected 5D (N, C, T, V, M), got {data.shape}\"\n",
    "                        assert area_id.dim() == 1, f\"Bad area_id shape {area_id.shape}\"\n",
    "                        if data.dim() == 5:\n",
    "                            data = data.unsqueeze(1)\n",
    "                        data = data.float().to(device)\n",
    "                        target = target.long().to(device)\n",
    "                        area_id = area_id.long().to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        out, _ = model(data, area_id)\n",
    "                        loss = criterion(out, target)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                        running_loss += loss.item() * data.size(0)\n",
    "                        preds = out.argmax(1)\n",
    "                        correct += preds.eq(target).sum().item()\n",
    "                        total += target.size(0)\n",
    "\n",
    "                    train_acc = correct / total\n",
    "                    logger.info(f\"Epoch {epoch+1}/{max_epoch} train_loss={running_loss/total:.4f} acc={train_acc:.4f}\")\n",
    "\n",
    "                    model.eval()\n",
    "                    eval_correct, eval_total = 0, 0\n",
    "                    with torch.no_grad():\n",
    "                        for data, target, _, area_id in eval_loader:\n",
    "                            assert data.dim() == 5, f\"Expected 5D (N, C, T, V, M), got {data.shape}\"\n",
    "                            assert area_id.dim() == 1, f\"Bad area_id shape {area_id.shape}\"\n",
    "                            if data.dim() == 5:\n",
    "                                data = data.unsqueeze(1)\n",
    "                            data = data.float().to(device)\n",
    "                            target = target.long().to(device)\n",
    "                            area_id = area_id.long().to(device)\n",
    "                            out, _ = model(data, area_id)\n",
    "                            preds = out.argmax(1)\n",
    "                            eval_correct += preds.eq(target).sum().item()\n",
    "                            eval_total += target.size(0)\n",
    "                    eval_acc = eval_correct / eval_total\n",
    "                    logger.info(f\"Eval acc={eval_acc:.4f}\")\n",
    "\n",
    "                    if eval_acc > best_acc:\n",
    "                        best_acc = eval_acc\n",
    "                        torch.save({\"model\": model.state_dict()}, os.path.join(save_dir, \"best.pth.tar\"))\n",
    "                        logger.info(f\"Saved best model (acc={best_acc:.4f})\")\n",
    "            else:\n",
    "                logger.info(\"Evaluate-only mode: skipping training\")\n",
    "\n",
    "            # Deterministic eval + save score\n",
    "            best_ckpt = os.path.join(save_dir, \"best.pth.tar\")\n",
    "            if os.path.exists(best_ckpt):\n",
    "                state = torch.load(best_ckpt, map_location=\"cpu\")\n",
    "                model.load_state_dict(state[\"model\"])\n",
    "                logger.info(\"Loaded best checkpoint\")\n",
    "            else:\n",
    "                logger.warning(\"Best checkpoint not found, using current weights\")\n",
    "\n",
    "            logger.info(\"Running final evaluation\")\n",
    "            model.eval()\n",
    "            all_logits, all_labels, all_names = [], [], []\n",
    "            with torch.no_grad():\n",
    "                for data, target, names, area_id in eval_loader:\n",
    "                    assert data.dim() == 5, f\"Expected 5D (N, C, T, V, M), got {data.shape}\"\n",
    "                    assert area_id.dim() == 1, f\"Bad area_id shape {area_id.shape}\"\n",
    "                    if data.dim() == 5:\n",
    "                        data = data.unsqueeze(1)\n",
    "                    data = data.float().to(device)\n",
    "                    target = target.long().to(device)\n",
    "                    area_id = area_id.long().to(device)\n",
    "                    out, _ = model(data, area_id)\n",
    "                    all_logits.append(out.cpu().numpy())\n",
    "                    all_labels.append(target.cpu().numpy())\n",
    "                    all_names.extend(names)\n",
    "\n",
    "            logits_arr = np.concatenate(all_logits, axis=0)\n",
    "            labels_arr = np.concatenate(all_labels, axis=0)\n",
    "            names_arr = np.array(all_names)\n",
    "\n",
    "            # Calculate final accuracy\n",
    "            preds = logits_arr.argmax(axis=1)\n",
    "            final_acc = (preds == labels_arr).mean()\n",
    "            \n",
    "            np.savez(os.path.join(args.work_dir, \"score_eval.npz\"), \n",
    "                    logits=logits_arr, labels=labels_arr, names=names_arr)\n",
    "            logger.info(f\"Saved score_eval.npz to {args.work_dir} (acc={final_acc:.4f})\")\n",
    "            \n",
    "            # Log captured output to file\n",
    "            logger.debug(log_capture.getvalue())\n",
    "            \n",
    "            # Print minimal info to notebook\n",
    "            print(f\"Completed fold {fold_id}, stream {stream_type} with accuracy: {final_acc:.4f}\")\n",
    "            \n",
    "            return args.work_dir, final_acc\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in run_stream: {e}\")\n",
    "            # Print error to notebook\n",
    "            print(f\"Error processing fold {fold_id}, stream {stream_type}: {e}\")\n",
    "            raise"
   ],
   "id": "e0306ba7bb415f33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fusion Function\n",
   "id": "7c15acc18cfc7949"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_fusion(fold_id, streams=ALL_STREAMS, weights=None):\n",
    "    \"\"\"Run late fusion for a specific fold using the ensemble script\n",
    "    \n",
    "    Args:\n",
    "        fold_id (str): Fold ID (e.g., \"06\")\n",
    "        streams (list): List of stream types to fuse\n",
    "        weights (list, optional): Weights for each stream\n",
    "        \n",
    "    Returns:\n",
    "        float: Fusion accuracy\n",
    "    \"\"\"\n",
    "    # Setup fusion-specific logger\n",
    "    log_file = os.path.join(LOGS_DIR, f\"fold_{fold_id}_fusion.log\")\n",
    "    logger = setup_logger(f'fold_{fold_id}_fusion', log_file)\n",
    "    \n",
    "    logger.info(f\"Running fusion for fold {fold_id}, streams: {streams}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if all score files exist\n",
    "        for stream in streams:\n",
    "            score_path = f\"{BASE_WORKDIR}/fold_{fold_id}_{stream}/score_eval.npz\"\n",
    "            if not os.path.exists(score_path):\n",
    "                error_msg = f\"Missing score file for fold {fold_id}, stream {stream}: {score_path}\"\n",
    "                logger.error(error_msg)\n",
    "                raise FileNotFoundError(error_msg)\n",
    "        \n",
    "        # Build command\n",
    "        cmd = [\n",
    "            \"python\", \"script/ensemble_playground.py\",\n",
    "            \"--fold\", fold_id,\n",
    "            \"--streams\"] + streams + [\n",
    "            \"--workdir\", BASE_WORKDIR,\n",
    "        ]\n",
    "        \n",
    "        if weights:\n",
    "            cmd.extend([\"--weights\"] + [str(w) for w in weights])\n",
    "        \n",
    "        # Run ensemble script\n",
    "        logger.info(f\"Running command: {' '.join(cmd)}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        logger.info(f\"Command output: {result.stdout}\")\n",
    "        if result.stderr:\n",
    "            logger.warning(f\"Command errors: {result.stderr}\")\n",
    "        \n",
    "        # Extract accuracy from output\n",
    "        accuracy = None\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if \"Accuracy:\" in line:\n",
    "                accuracy = float(line.split()[-1])\n",
    "                logger.info(f\"Extracted accuracy from output: {accuracy}\")\n",
    "        \n",
    "        # If we couldn't extract from output, read from the JSON file\n",
    "        if accuracy is None:\n",
    "            fusion_json = f\"{BASE_WORKDIR}/fold_{fold_id}_ensemble/fused_score_eval_fold{fold_id}.json\"\n",
    "            if os.path.exists(fusion_json):\n",
    "                with open(fusion_json, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    accuracy = data['metrics']['accuracy']\n",
    "                    logger.info(f\"Extracted accuracy from JSON: {accuracy}\")\n",
    "        \n",
    "        if accuracy is None:\n",
    "            logger.error(\"Could not determine fusion accuracy\")\n",
    "            raise ValueError(\"Could not determine fusion accuracy\")\n",
    "        \n",
    "        # Print minimal info to notebook\n",
    "        print(f\"Completed fusion for fold {fold_id} with accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return accuracy\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error in run_fusion: {e}\")\n",
    "        # Print error to notebook\n",
    "        print(f\"Error running fusion for fold {fold_id}: {e}\")\n",
    "        raise"
   ],
   "id": "a37bc7a3d40e980b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## In-Notebook Fusion Function (Alternative)\n",
   "id": "d83fbc0ca16e0497"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_notebook_fusion(fold_id, streams=ALL_STREAMS, weights=None):\n",
    "    \"\"\"Run late fusion for a specific fold directly in the notebook\n",
    "    \n",
    "    Args:\n",
    "        fold_id (str): Fold ID (e.g., \"06\")\n",
    "        streams (list): List of stream types to fuse\n",
    "        weights (list, optional): Weights for each stream\n",
    "        \n",
    "    Returns:\n",
    "        float: Fusion accuracy\n",
    "    \"\"\"\n",
    "    # Setup fusion-specific logger\n",
    "    log_file = os.path.join(LOGS_DIR, f\"fold_{fold_id}_notebook_fusion.log\")\n",
    "    logger = setup_logger(f'fold_{fold_id}_notebook_fusion', log_file)\n",
    "    \n",
    "    logger.info(f\"Running in-notebook fusion for fold {fold_id}, streams: {streams}\")\n",
    "    \n",
    "    try:\n",
    "        if weights is None:\n",
    "            weights = [1.0] * len(streams)\n",
    "        weights = np.array(weights) / sum(weights)\n",
    "        logger.info(f\"Using weights: {weights}\")\n",
    "        \n",
    "        scores = []\n",
    "        labels_ref = None\n",
    "        names_ref = None\n",
    "        \n",
    "        for s in streams:\n",
    "            path = f\"{BASE_WORKDIR}/fold_{fold_id}_{s}/score_eval.npz\"\n",
    "            logger.info(f\"Loading scores from {path}\")\n",
    "            d = np.load(path)\n",
    "            if labels_ref is None:\n",
    "                labels_ref = d[\"labels\"]\n",
    "                names_ref = d[\"names\"]\n",
    "                logger.info(f\"Reference labels shape: {labels_ref.shape}\")\n",
    "            else:\n",
    "                if not np.array_equal(labels_ref, d[\"labels\"]):\n",
    "                    error_msg = \"Label mismatch across streams\"\n",
    "                    logger.error(error_msg)\n",
    "                    raise ValueError(error_msg)\n",
    "                if not np.array_equal(names_ref, d[\"names\"]):\n",
    "                    error_msg = \"Name/order mismatch across streams\"\n",
    "                    logger.error(error_msg)\n",
    "                    raise ValueError(error_msg)\n",
    "            scores.append(d[\"logits\"])\n",
    "            logger.info(f\"Loaded logits shape: {d['logits'].shape}\")\n",
    "\n",
    "        # Weighted fusion\n",
    "        logger.info(\"Performing weighted fusion\")\n",
    "        fused = np.zeros_like(scores[0])\n",
    "        for score, weight in zip(scores, weights):\n",
    "            fused += score * weight\n",
    "            \n",
    "        preds = fused.argmax(1)\n",
    "        final_acc = (preds == labels_ref).mean()\n",
    "        logger.info(f\"In-notebook fusion accuracy: {final_acc:.4f}\")\n",
    "        \n",
    "        # Save fusion results\n",
    "        fusion_dir = f\"{BASE_WORKDIR}/fold_{fold_id}_ensemble\"\n",
    "        os.makedirs(fusion_dir, exist_ok=True)\n",
    "        output_path = os.path.join(fusion_dir, f\"notebook_fused_logits_fold{fold_id}.npz\")\n",
    "        logger.info(f\"Saving fusion results to {output_path}\")\n",
    "        np.savez(\n",
    "            output_path,\n",
    "            logits=fused,\n",
    "            labels=labels_ref,\n",
    "            names=names_ref,\n",
    "            preds=preds,\n",
    "        )\n",
    "        \n",
    "        # Print minimal info to notebook\n",
    "        print(f\"Completed in-notebook fusion for fold {fold_id} with accuracy: {final_acc:.4f}\")\n",
    "        \n",
    "        return final_acc\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error in run_notebook_fusion: {e}\")\n",
    "        # Print error to notebook\n",
    "        print(f\"Error running in-notebook fusion for fold {fold_id}: {e}\")\n",
    "        raise"
   ],
   "id": "6213a9953770d6a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Master Pipeline Function with Cooldowns\n",
   "id": "7e48ca84549f4aac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_master_pipeline(folds=ALL_FOLDS, streams=ALL_STREAMS, evaluate_only=EVALUATE_ONLY):\n",
    "    \"\"\"Run the complete pipeline for all folds and streams with cooldown periods\n",
    "    \n",
    "    Args:\n",
    "        folds (list): List of fold IDs to process\n",
    "        streams (list): List of stream types to process\n",
    "        evaluate_only (bool): If True, skip training and just evaluate\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Summary of results for all folds\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    main_logger.info(f\"Starting master pipeline at {start_time}\")\n",
    "    main_logger.info(f\"Processing {len(folds)} folds: {folds}\")\n",
    "    main_logger.info(f\"Processing {len(streams)} streams: {streams}\")\n",
    "    main_logger.info(f\"Evaluate only mode: {evaluate_only}\")\n",
    "    main_logger.info(f\"Using 5-minute cooldown between folds\")\n",
    "    main_logger.info(f\"Using 1-minute cooldown between streams\")\n",
    "    \n",
    "    # Print minimal info to notebook\n",
    "    print(f\"Starting pipeline at {start_time}\")\n",
    "    print(f\"Processing {len(folds)} folds with 5-minute cooldown between folds\")\n",
    "    print(f\"Processing {len(streams)} streams with 1-minute cooldown between streams\")\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    results = {}\n",
    "    \n",
    "    # Process each fold (one at a time with cooldown)\n",
    "    for i, fold_id in enumerate(folds):\n",
    "        fold_start_time = datetime.now()\n",
    "        main_logger.info(f\"Processing fold {fold_id} (started at {fold_start_time})\")\n",
    "        print(f\"Processing fold {fold_id} ({i+1}/{len(folds)}) started at {fold_start_time}\")\n",
    "        \n",
    "        # Initialize fold results\n",
    "        results[fold_id] = {\n",
    "            \"streams\": {},\n",
    "            \"fusion\": None,\n",
    "            \"work_dirs\": []\n",
    "        }\n",
    "        \n",
    "        # Process each stream for this fold (with cooldown between streams)\n",
    "        for j, stream in enumerate(streams):\n",
    "            stream_start_time = datetime.now()\n",
    "            main_logger.info(f\"Processing fold {fold_id}, stream {stream} (started at {stream_start_time})\")\n",
    "            print(f\"  Processing stream {stream} ({j+1}/{len(streams)})\")\n",
    "            \n",
    "            try:\n",
    "                work_dir, accuracy = run_stream(fold_id, stream, evaluate_only)\n",
    "                results[fold_id][\"streams\"][stream] = accuracy\n",
    "                results[fold_id][\"work_dirs\"].append(work_dir)\n",
    "                main_logger.info(f\"Completed fold {fold_id}, stream {stream} with accuracy: {accuracy:.4f}\")\n",
    "            except Exception as e:\n",
    "                main_logger.error(f\"Error processing fold {fold_id}, stream {stream}: {e}\")\n",
    "                results[fold_id][\"streams\"][stream] = None\n",
    "            \n",
    "            # Add 1-minute cooldown between streams (except after the last stream)\n",
    "            if j < len(streams) - 1:\n",
    "                main_logger.info(f\"Cooldown: waiting 1 minute before next stream\")\n",
    "                print(f\"  Cooldown: waiting 1 minute before next stream...\")\n",
    "                time.sleep(60)  # 1 minute in seconds\n",
    "        \n",
    "        # Run fusion if all streams were processed successfully\n",
    "        if all(results[fold_id][\"streams\"].get(s) is not None for s in streams):\n",
    "            try:\n",
    "                # Try script-based fusion first\n",
    "                fusion_acc = run_fusion(fold_id, streams)\n",
    "                results[fold_id][\"fusion\"] = fusion_acc\n",
    "                main_logger.info(f\"Completed fusion for fold {fold_id} with accuracy: {fusion_acc:.4f}\")\n",
    "            except Exception as e:\n",
    "                main_logger.error(f\"Error running script fusion for fold {fold_id}: {e}\")\n",
    "                try:\n",
    "                    # Fall back to in-notebook fusion\n",
    "                    main_logger.info(\"Falling back to in-notebook fusion\")\n",
    "                    print(\"  Falling back to in-notebook fusion...\")\n",
    "                    fusion_acc = run_notebook_fusion(fold_id, streams)\n",
    "                    results[fold_id][\"fusion\"] = fusion_acc\n",
    "                    main_logger.info(f\"Completed in-notebook fusion for fold {fold_id} with accuracy: {fusion_acc:.4f}\")\n",
    "                except Exception as e2:\n",
    "                    main_logger.error(f\"Error running in-notebook fusion for fold {fold_id}: {e2}\")\n",
    "                    results[fold_id][\"fusion\"] = None\n",
    "        \n",
    "        fold_end_time = datetime.now()\n",
    "        fold_duration = fold_end_time - fold_start_time\n",
    "        main_logger.info(f\"Completed fold {fold_id} in {fold_duration} (ended at {fold_end_time})\")\n",
    "        print(f\"Completed fold {fold_id} in {fold_duration}\")\n",
    "        \n",
    "        # Save intermediate results after each fold\n",
    "        intermediate_results_file = os.path.join(RESULTS_DIR, f\"fusion_results_intermediate_{timestamp}.json\")\n",
    "        with open(intermediate_results_file, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        main_logger.info(f\"Saved intermediate results to {intermediate_results_file}\")\n",
    "        \n",
    "        # Add 5-minute cooldown between folds (except after the last fold)\n",
    "        if i < len(folds) - 1:\n",
    "            main_logger.info(f\"Cooldown: waiting 5 minutes before next fold\")\n",
    "            print(f\"Cooldown: waiting 5 minutes before next fold...\")\n",
    "            time.sleep(300)  # 5 minutes in seconds\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    summary_data = []\n",
    "    for fold_id in folds:\n",
    "        if fold_id in results:\n",
    "            row = {\"Fold\": fold_id}\n",
    "            for stream in streams:\n",
    "                row[f\"{stream}_Acc\"] = results[fold_id][\"streams\"].get(stream)\n",
    "            row[\"Fusion_Acc\"] = results[fold_id][\"fusion\"]\n",
    "            summary_data.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Save results to JSON\n",
    "    results_file = os.path.join(RESULTS_DIR, f\"fusion_results_{timestamp}.json\")\n",
    "    with open(results_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    main_logger.info(f\"Saved detailed results to {results_file}\")\n",
    "    print(f\"Saved detailed results to {results_file}\")\n",
    "    \n",
    "    # Save summary to CSV\n",
    "    summary_file = os.path.join(RESULTS_DIR, f\"fusion_summary_{timestamp}.csv\")\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    main_logger.info(f\"Saved summary to {summary_file}\")\n",
    "    print(f\"Saved summary to {summary_file}\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    total_duration = end_time - start_time\n",
    "    main_logger.info(f\"Master pipeline completed in {total_duration} (started: {start_time}, ended: {end_time})\")\n",
    "    print(f\"Master pipeline completed in {total_duration}\")\n",
    "    print(f\"Started: {start_time}, ended: {end_time}\")\n",
    "    \n",
    "    return summary_df"
   ],
   "id": "769b65acbc69bebe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run Pipeline for All Folds and Streams\n",
   "id": "46371db1c9fe8cfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run the complete pipeline\n",
    "summary_df = run_master_pipeline()"
   ],
   "id": "6759729ae0ea6d32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Find Best Fold\n",
   "id": "e4b1e8d007c60281"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sort by fusion accuracy to find the best fold\n",
    "sorted_df = summary_df.sort_values(by=\"Fusion_Acc\", ascending=False).reset_index(drop=True)\n",
    "print(\"Folds sorted by fusion accuracy (best to worst):\")\n",
    "display(sorted_df)\n",
    "\n",
    "# Get the best fold\n",
    "best_fold = sorted_df.iloc[0][\"Fold\"]\n",
    "best_fusion_acc = sorted_df.iloc[0][\"Fusion_Acc\"]\n",
    "\n",
    "print(f\"\\nBest fold: {best_fold} with fusion accuracy: {best_fusion_acc:.4f}\")\n",
    "print(f\"Stream accuracies for best fold:\")\n",
    "for stream in ALL_STREAMS:\n",
    "    acc = sorted_df.iloc[0][f\"{stream}_Acc\"]\n",
    "    print(f\"  {stream}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nBest fold score files:\")\n",
    "for stream in ALL_STREAMS:\n",
    "    print(f\"  {stream}: {BASE_WORKDIR}/fold_{best_fold}_{stream}/score_eval.npz\")\n",
    "print(f\"  Fusion: {BASE_WORKDIR}/fold_{best_fold}_ensemble/fused_score_eval_fold{best_fold}.json\")"
   ],
   "id": "9fe6d1238eb59868"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run Pipeline for Specific Folds (Optional)\n",
   "id": "ffb283d4bc2ad69b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example: Run only specific folds\n",
    "# selected_folds = [\"00\", \"01\", \"02\"]\n",
    "# summary_df_subset = run_master_pipeline(folds=selected_folds)"
   ],
   "id": "d7cd71c0a72c0428"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run Evaluation Only (Optional)\n",
   "id": "5967107d0c2ec176"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example: Run evaluation only (no training) for all folds\n",
    "# summary_df_eval = run_master_pipeline(evaluate_only=True)"
   ],
   "id": "ceaacdba36bbbc6b"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
